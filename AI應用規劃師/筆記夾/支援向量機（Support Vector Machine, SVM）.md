### **支援向量機（Support Vector Machine, SVM）基礎知識**

支援向量機（SVM）是一種**監督式學習（Supervised Learning）**的分類模型，主要用於**二元分類（Binary Classification）**，但也可透過調整擴展為多分類與迴歸（SVR, Support Vector Regression）。

---

### **🔹 SVM 的核心概念**

1. **超平面（Hyperplane）**
    
    - 在 nnn 維空間中，SVM 嘗試找到一個最佳的「超平面」來分隔不同類別的數據點。
    - 對於二維數據，這條超平面是一條直線；對於三維數據，它是一個平面。
2. **最大間隔（Margin）**
    
    - SVM 透過找到「**間隔（Margin）**」最大的超平面來分類資料。
    - 間隔是指超平面與**距離最近的樣本（稱為支援向量, Support Vectors）**之間的距離。
    - 這樣的設計讓 SVM 在新數據來時有更好的泛化能力。
3. **支援向量（Support Vectors）**
    
    - 距離超平面最近的數據點，即「支撐」超平面的數據點，對分類結果影響最大。
    - 只有這些支援向量影響最終的決策邊界，其他數據點則不影響訓練結果。
4. **硬間隔與軟間隔（Hard Margin vs. Soft Margin）**
    
    - **硬間隔（Hard Margin）**：適用於線性可分的數據，不允許誤分類。
    - **軟間隔（Soft Margin）**：允許部分數據點違規（誤分類），透過 **懲罰參數 CCC** 控制錯誤數量。
5. **核技巧（Kernel Trick）**
    
    - 當數據**線性不可分**時，SVM 使用「核函數（Kernel Function）」將數據映射到更高維度，使其變得可分。
    - **常見的核函數**：
        - **線性核（Linear Kernel）**：適用於線性可分數據。
        - **多項式核（Polynomial Kernel）**：適用於較複雜的邊界。
        - **高斯徑向基核（RBF Kernel）**：最常用，適用於非線性數據。
        - **Sigmoid 核**：類似於神經網絡中的激活函數。

---

### **📌 SVM 的數學表達**

1. **最佳超平面方程**
    
    ![[最佳超平面.png]]
2. **優化目標** SVM 的目標是最大化間隔，這可以透過以下優化問題來表示：
    
    ![[SVM 優化問題.png]]
3. **引入鬆弛變數（軟間隔）** 當數據線性不可分時，引入鬆弛變數 ξi\xi_iξi​ 來允許誤分類：
    
    ![[引入鬆弛變數.png]]
    
    其中 CCC 是調節錯誤數量的懲罰係數。
    

---

### **📌 SVM 的考點**

✅ **超平面與最大間隔概念**  
✅ **支援向量與其影響**  
✅ **硬間隔 vs. 軟間隔的區別**  
✅ **核函數及選擇（Kernel Trick）**  
✅ **SVM 的優化公式（最大間隔）**  
✅ **SVM 的應用領域（文本分類、影像分類等）**

---

### **📌 SVM 的應用場景**

- **文本分類**（如垃圾郵件檢測）
- **人臉識別**
- **手寫數字辨識**
- **生物信息學（DNA 序列分類）**

**👉 總結**

- SVM 是一種基於 **最大間隔** 的分類器，適合小數據集且對高維數據效果良好。
- **核技巧（Kernel Trick）** 使 SVM 具備處理非線性分類問題的能力。
- 主要考點包含超平面、支援向量、軟間隔、核函數選擇與優化目標。

🚀 **適合需要高泛化能力的分類問題，特別是高維、小樣本數據集！**