### 梯度提升機（Gradient Boosting Machine, GBM）的基礎知識與考點

**梯度提升機（GBM）** 是一種強大的 **集成學習** 算法，屬於 **提升法（Boosting）** 類型，它通過多個弱學習器（通常是決策樹）的加權組合來提升模型的預測能力。GBM 通常被用來解決分類和回歸問題。

#### 基本概念

GBM 主要基於以下兩個核心理念：

1. **弱學習器**：GBM 通常使用簡單的模型（例如，淺層決策樹），這些模型通常表現得不好，但在與其他模型結合後，能夠顯著提高預測準確性。
    
2. **加法模型**：GBM 使用加法模型來進行預測，即將多個弱學習器的預測結果加權合併，形成最終預測結果。
    

#### 工作原理

1. **初始化**：首先，GBM 會使用一個基準模型（通常是均值或中位數）來初始化預測結果。這通常對應於回歸問題中的平均值，對分類問題則是類別的比例。
    
2. **迭代訓練弱學習器**：
    
    - 在每一輪迭代中，GBM 會計算當前模型的 **殘差（Residuals）**，即模型預測與實際值之間的差異。
    - 然後，一個新的決策樹被訓練來預測這些殘差。
3. **梯度下降**：
    
    - 每棵新樹都會對前一棵樹的錯誤進行修正。
    - 在每一輪訓練中，GBM 會根據梯度下降的原理，調整弱學習器的預測，以最小化損失函數。這個過程通常會逐步減小錯誤，直到達到設定的停止條件（如最大迭代次數或最小誤差）。
4. **最終模型**：最終，GBM 的預測是所有弱學習器預測的加權和。
    

#### 數學形式

![[梯度提升機 數學形式.png]]

#### 主要參數與調整

1. **學習率（Learning Rate）**：
    
    - 學習率控制每棵新樹對最終預測的貢獻程度。
    - 小學習率會讓模型訓練得更穩定，但需要更多的迭代次數。
2. **樹的深度（Tree Depth）**：
    
    - 樹的深度影響模型的複雜度。較淺的樹有較少的決策節點，較深的樹則能夠擬合更複雜的模式。
3. **迭代次數（Number of Estimators）**：
    
    - 這是模型中弱學習器的數量。過多的樹可能會導致過擬合，過少的樹則可能無法捕捉數據中的模式。
4. **子樣本比例（Subsample）**：
    
    - GBM 允許在每輪迭代中使用訓練數據的隨機子集來訓練每顆樹，這樣可以減少過擬合。

#### 優缺點

**優點**：

1. **強大的性能**：GBM 能夠處理各種複雜的非線性關係，且表現通常非常好。
2. **適用於各種問題**：不僅可以處理回歸問題，也適用於分類問題。
3. **特徵重要性評估**：可以通過觀察每顆樹中的特徵分裂來評估特徵的重要性。

**缺點**：

1. **過擬合風險**：如果不進行適當的正則化（如控制樹的深度、調整學習率等），GBM 容易過擬合。
2. **訓練時間長**：由於每一輪都需要訓練多顆樹，因此訓練時間可能較長，尤其是在大數據集上。
3. **對異常值敏感**：GBM 可能會對異常值過度擬合，因此需要進行適當的數據預處理。

#### 主要應用

- **分類**：在許多競賽中，GBM 被廣泛應用於二分類和多分類問題。例如，金融詐騙檢測、垃圾郵件分類等。
- **回歸**：GBM 也常應用於回歸問題，如預測房價、銷售額等。
- **推薦系統**：可以利用 GBM 來預測用戶的偏好和行為。
- **時間序列預測**：在一些非線性的時間序列預測問題中，GBM 也能提供優異的表現。

#### 考點

- **GBM 的工作原理與訓練過程**。
- **如何選擇和調整 GBM 的超參數**（如學習率、樹深、迭代次數等）。
- **GBM 的優缺點及其適用場景**。
- **GBM 與其他集成方法（如隨機森林、AdaBoost）的比較**。

總結來說，GBM 是一個強大的模型，能夠處理複雜的預測問題，但需要謹慎調參以避免過擬合，並在需要的時候對訓練時間進行優化。