### **超參數（Hyperparameters）**

超參數是機器學習和深度學習模型中的「可調參數」，它們在模型訓練之前設置，影響模型的學習過程和最終效果。與之相對的，**模型參數（Parameters）** 是在訓練過程中透過數據學習得到的，例如神經網路中的權重（weights）和偏置（biases）。

---

## **1. 超參數的分類**

超參數可以大致分為以下幾類：

### **(1) 模型結構相關的超參數**

這些超參數決定了模型的結構，通常在設計時固定。

- **隱藏層數（Number of Hidden Layers）**：影響模型的複雜度，如深度神經網路（DNN）的層數。
- **每層神經元數量（Number of Neurons per Layer）**：決定模型的表達能力。
- **激活函數（Activation Function）**：如 ReLU、Sigmoid、Tanh，影響神經元的輸出範圍。

### **(2) 優化相關的超參數**

這些超參數影響模型的學習過程。

- **學習率（Learning Rate, `α`）**：控制模型更新權重的步伐，值過大容易震盪不收斂，值過小則收斂太慢。
- **批次大小（Batch Size）**：決定一次訓練所用的樣本數，影響計算效率和收斂速度。
- **優化器（Optimizer）**：如 SGD、Adam、RMSprop，影響梯度下降的方式。
- **權重初始化（Weight Initialization）**：影響訓練開始時的收斂性，例如 Xavier、He 初始化。

### **(3) 正則化相關的超參數**

這些超參數用於防止過擬合（Overfitting）。

- **L1/L2 正則化（Lasso / Ridge Regularization）**：調整模型的權重大小，防止過擬合。
- **Dropout 比例（Dropout Rate）**：防止神經網路過度依賴部分神經元，提高泛化能力。
- **早停（Early Stopping）**：監測驗證集損失，當模型表現開始惡化時停止訓練。

### **(4) 特定模型的超參數**

不同類型的模型會有特定的超參數，例如：

- **決策樹（Decision Tree）**
    - 樹的最大深度（Max Depth）
    - 最小葉節點數（Min Samples Leaf）
    - 最大特徵數（Max Features）
- **支持向量機（SVM）**
    - 正則化參數 `C`
    - 核函數（Kernel Function）：線性、RBF、多項式等
    - RBF 核的 `γ`（Gamma）
- **K-Means**
    - 簇數 `k`
    - 初始化方式（如 k-means++）

---

## **2. 超參數調整（Hyperparameter Tuning）**

由於超參數對模型影響很大，因此需要進行調整，以獲得最佳的模型表現。

### **(1) 網格搜索（Grid Search）**

- 定義一組超參數組合，遍歷所有可能的配置，找到表現最好的組合。
- 缺點：計算成本高，尤其是超參數數量多時。

### **(2) 隨機搜索（Random Search）**

- 隨機選取超參數組合進行測試，比網格搜索更高效，適合高維度超參數空間。
- 優點：通常比網格搜索快，但仍然需要大量試驗。

### **(3) 貝葉斯優化（Bayesian Optimization）**

- 使用貝葉斯定理來選擇最有希望的超參數組合，而不是隨機或網格搜索。
- 適用於計算成本高的深度學習模型。

### **(4) 超參數自適應調整（Hyperband / BOHB）**

- Hyperband：根據評估結果動態調整資源，快速篩選出較好的組合。
- BOHB（Bayesian Optimization + Hyperband）：結合貝葉斯優化和 Hyperband，提高調參效率。

---

## **3. 超參數與模型參數的比較**

|類別|超參數（Hyperparameter）|模型參數（Model Parameter）|
|---|---|---|
|**定義方式**|事先設定|透過訓練學習得到|
|**影響範圍**|影響學習過程|影響模型預測結果|
|**舉例**|學習率、批次大小、隱藏層數|權重（Weights）、偏置（Bias）|

---

## **4. 超參數調整的實務考點**

1. **學習率調整**
    
    - 太高 → 訓練不穩定，可能發散
    - 太低 → 訓練收斂過慢，可能卡在局部最優
    - 一般做法：**學習率衰減（Learning Rate Decay）**
2. **批次大小**
    
    - 小批次（如 32）：較好的泛化能力，但計算較慢
    - 大批次（如 512）：訓練更快，但可能過擬合
3. **正則化超參數**
    
    - `L2` 正則化可以幫助降低過擬合風險
    - `Dropout` 介於 0.2~0.5 常用於防止過擬合
4. **選擇最適合的超參數調整方法**
    
    - 若超參數空間小，可用 **Grid Search**
    - 若超參數空間大，優先用 **Random Search**
    - 深度學習建議用 **貝葉斯優化 / Hyperband**

---

### **總結**

- **超參數影響模型的學習過程**，包括網路結構（層數、神經元數）、優化（學習率、批次大小）及正則化（L1/L2、Dropout）。
- **超參數需透過調整找到最佳組合**，常見方法有網格搜索、隨機搜索、貝葉斯優化等。
- **調整學習率、批次大小、正則化方式可影響模型的收斂速度與泛化能力**。

這些考點在 AI、機器學習、深度學習考試和面試中經常出現，熟悉不同的超參數調整方法能提升模型性能並優化學習過程。