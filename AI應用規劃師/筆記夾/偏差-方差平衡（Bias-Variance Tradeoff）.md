## **偏差-方差平衡（Bias-Variance Tradeoff）基礎知識**

偏差-方差平衡（Bias-Variance Tradeoff）是機器學習中影響模型泛化能力的核心概念，主要描述**模型的誤差來源**，並解釋為何選擇適當的模型複雜度能提升預測效果。

---

### **🔹 1. 偏差（Bias）**

- **定義**：指模型對訓練數據的適配能力，代表模型對數據的假設程度。
- **高偏差（High Bias）**：
    - 模型過於簡單，無法有效學習數據模式，導致**欠擬合（Underfitting）**。
    - 例如：線性回歸在處理非線性數據時，可能產生高偏差問題。
- **低偏差（Low Bias）**：
    - 模型能夠較好地適應數據分佈，對數據擬合較佳。

#### **數學表達**

預測誤差的偏差分量可以表示為：

Bias2=(E[f^(x)]−f(x))2\text{Bias}^2 = (\mathbb{E}[\hat{f}(x)] - f(x))^2Bias2=(E[f^​(x)]−f(x))2

其中：

- f^(x)\hat{f}(x)f^​(x) 是模型的預測函數。
- f(x)f(x)f(x) 是真實的數據分佈函數。

---

### **🔹 2. 方差（Variance）**

- **定義**：衡量模型對不同訓練集的敏感程度，即模型對輸入數據的變化程度。
- **高方差（High Variance）**：
    - 模型過於複雜，容易受訓練數據的細節影響，導致**過擬合（Overfitting）**。
    - 例如：深度決策樹可能會產生高方差問題，對訓練數據的噪聲過度學習。
- **低方差（Low Variance）**：
    - 模型對不同訓練集的變動不敏感，較能泛化至新數據。

#### **數學表達**

方差分量可以表示為：

Variance=E[(f^(x)−E[f^(x)])2]\text{Variance} = \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]Variance=E[(f^​(x)−E[f^​(x)])2]

表示模型預測結果與其期望值的變異程度。

---

### **🔹 3. 偏差-方差分解（Bias-Variance Decomposition）**

總體預測誤差可分解為**偏差、方差與不可約誤差（Irreducible Error）**：

E[(y−f^(x))2]=Bias2+Variance+σ2\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \sigma^2E[(y−f^​(x))2]=Bias2+Variance+σ2

其中：

- **Bias2\text{Bias}^2Bias2**：模型與真實函數的誤差（系統性誤差）。
- **Variance\text{Variance}Variance**：模型對不同訓練集的變動程度。
- **σ2\sigma^2σ2**：不可約誤差（數據內在的噪聲，無法降低）。

📌 **偏差與方差的關係**

- **高偏差時**，模型簡單但可能欠擬合，無法有效學習數據。
- **高方差時**，模型複雜但可能過擬合，對新數據泛化能力較差。
- **理想情況**：透過適當調整模型，使偏差與方差達到平衡，獲得最小誤差。

---

### **📌 4. 偏差-方差平衡與模型選擇**

- **簡單模型（如線性回歸）** → 偏差高、方差低，可能欠擬合。
- **複雜模型（如深度神經網絡）** → 偏差低、方差高，可能過擬合。
- **最佳模型** → 在偏差與方差之間取得平衡，使總誤差最小。

---

### **📌 5. 偏差-方差平衡的應用與考點**

✅ **理解偏差與方差的定義與影響**  
✅ **理解偏差-方差分解公式及其應用**  
✅ **過擬合（Overfitting）與欠擬合（Underfitting）的區別**  
✅ **如何調整模型來改善泛化能力（調整正則化、特徵選擇等）**  
✅ **如何根據問題選擇適當的模型（如深度 vs. 淺層模型）**

---

### **📌 6. 如何調整偏差與方差？**

**➡ 增加偏差、降低方差（避免過擬合）**

- **減少模型複雜度**（例如減少神經網絡層數或決策樹深度）。
- **加入正則化（L1/L2 正則化）**，抑制模型對特徵的過度學習。
- **增加訓練數據**，讓模型更好地學習數據分佈。

**➡ 降低偏差、增加方差（避免欠擬合）**

- **使用更複雜的模型**（例如從線性回歸切換到神經網絡）。
- **增加特徵數量**，提供模型更多資訊來學習。
- **降低正則化強度**，允許模型更靈活地學習數據。

📌 **最終目標：找到適合的偏差-方差平衡點，使模型泛化能力最佳！** 🚀