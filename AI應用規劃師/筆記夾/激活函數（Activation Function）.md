### 激活函數（Activation Function）的基礎知識

激活函數是神經網絡中至關重要的組成部分，它將神經元的輸入信號轉換為輸出信號，並且通常會加入非線性因素，使得神經網絡能夠學習複雜的模式和關係。

#### 1. **激活函數的基本作用**：

- **非線性變換**：使得神經網絡能夠學習非線性關係，從而能夠處理更複雜的任務。
- **限制輸出範圍**：激活函數的範圍通常限制在一個區間內，這樣有助於避免過大的數值範圍對模型訓練的影響。

#### 2. **常見的激活函數**

1. **Sigmoid 函數（邏輯斯迴歸函數）**：
    
    - **公式**： $f(x) = \frac{1}{1 + e^{-x}}$
    - **範圍**：(0,1)(0, 1)(0,1)。
    - **用途**：適用於二分類問題，輸出可以解釋為概率。
    - **缺點**：當輸入過大或過小時，梯度消失問題會影響訓練速度。
2. **Tanh（雙曲正切函數）**：
    
    - **公式**： $f(x) = \tanh(x) = \frac{2}{1 + e^{-2x}} - 1$
    - **範圍**：(−1,1)(-1, 1)(−1,1)。
    - **用途**：與 Sigmoid 相似，範圍更大，適用於處理零中心化的數據。
    - **缺點**：仍然會有梯度消失問題。
3. **ReLU（修正線性單元）**：
    
    - **公式**：$f(x) = \max(0, x)$
    - **範圍**：[0,+∞)[0, +\infty)[0,+∞)。
    - **用途**：常用於隱藏層，簡單且高效，能夠有效減少梯度消失問題。
    - **缺點**：當 x<0x < 0x<0 時，梯度為零，可能會導致部分神經元不再更新（即“死神經元”問題）。
4. **Leaky ReLU**：
    
    - **公式**： $f(x) = \max(\alpha x, x) \quad (\alpha \text{ 是一個小常數})$
    - **範圍**：(−∞,+∞)(-\infty, +\infty)(−∞,+∞)。
    - **用途**：用來解決 ReLU 的死神經元問題，即使 x<0x < 0x<0 時，也會有小的梯度更新。
    - **缺點**：可能會對某些網絡結構的訓練不太有效。
5. **Softmax 函數**：
    
    - **公式**：$f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$
    - **範圍**：(0,1)(0, 1)(0,1)，且所有輸出的總和為 1。
    - **用途**：常用於多分類問題的輸出層，將每個神經元的輸出轉換為概率。
6. **Swish 函數**：
    
    - **公式**：$f(x) = x \cdot \sigma(x) = x \cdot \frac{1}{1 + e^{-x}}$
    - **範圍**：(−∞,+∞)(-\infty, +\infty)(−∞,+∞)。
    - **用途**：由 Google 提出的新激活函數，對於某些情況下，Swish 的表現比 ReLU 更好。
    - **優點**：較少出現死神經元問題，並且能在一定程度上改善訓練過程。

#### 3. **激活函數的選擇原則**

- **ReLU 或其變種（Leaky ReLU、ELU、Swish）**：通常選擇這些激活函數作為隱藏層的激活函數，因為它們能有效處理梯度消失問題。
- **Sigmoid 和 Tanh**：這些激活函數一般用於較簡單的模型，或者在需要範圍限制時使用。
- **Softmax**：通常用於多分類問題的輸出層。

#### 4. **考試重點**

- 激活函數的數學公式及範圍。
- 每種激活函數的適用場景與優缺點。
- 激活函數在訓練過程中的作用，例如 ReLU 如何解決梯度消失問題，Softmax 如何處理多分類問題。
- 激活函數選擇的影響，尤其是在深度神經網絡中的重要性。

