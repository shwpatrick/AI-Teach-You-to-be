**K近鄰算法（K-Nearest Neighbors, K-NN）** 是一種簡單且直觀的監督學習算法，廣泛應用於分類和回歸問題。其基本思想是，對於一個新的樣本，根據它與訓練集中最相似的K個鄰居的標籤來預測其標籤。

**K值**指的是在進行預測時，用來決定“鄰居”的數量。具體來說，對於一個新的樣本，K-NN會計算該樣本與訓練集中的所有樣本的距離，然後選擇距離最近的**K個樣本**，用來推斷該樣本的類別（分類問題）或數值（回歸問題）。

### 基本概念

1. **K近鄰算法的工作原理**：
    
    - **分類問題**：對於一個新的樣本，K-NN會計算該樣本與訓練集中的每一個樣本之間的距離，然後選擇與該樣本距離最近的K個樣本。根據這些K個鄰居的標籤，使用**多數表決法**（majority voting）來決定新樣本的標籤。
    - **回歸問題**：K-NN也可以用來解決回歸問題。在這種情況下，預測值是K個鄰居標籤的**均值**或**加權均值**。
2. **距離度量**：
    
    - K-NN算法的核心是計算樣本之間的距離，常見的距離度量方法包括：
        - **歐氏距離（Euclidean Distance）**：對於兩個點 (x1,x2,...,xn)(x_1, x_2, ..., x_n) 和 (y1,y2,...,yn)(y_1, y_2, ..., y_n)，歐氏距離定義為： d(x,y)=∑i=1n(xi−yi)2d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
        - **曼哈頓距離（Manhattan Distance）**：定義為樣本在各維度上差異的總和： d(x,y)=∑i=1n∣xi−yi∣d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} |x_i - y_i|
        - **餘弦相似度（Cosine Similarity）**：衡量兩個向量之間的角度差異，適用於文本和高維數據。
3. **K值的選擇**：
    
    - **K值**是K-NN算法中的一個重要超參數，它控制了鄰居的數量。選擇較小的K會使模型對噪聲敏感，容易過擬合；選擇較大的K則可能使得模型過於平滑，從而欠擬合。
    - 通常通過交叉驗證來選擇合適的K值。
4. **算法步驟**：
    
    - 訓練階段：K-NN算法不需要明確的訓練過程。訓練數據集僅用來計算距離。
    - 測試階段：對於每個測試樣本，計算其與所有訓練樣本的距離，選擇最近的K個樣本並根據它們的標籤進行預測。

### 優缺點

**優點**：

- **簡單直觀**：K-NN算法非常直觀，易於理解和實現。
- **無需訓練**：K-NN是一種懶學習算法，不需要訓練過程，所有計算都在預測階段進行。
- **非參數方法**：K-NN不需要對數據進行任何假設，對於非線性問題效果較好。

**缺點**：

- **計算代價高**：由於每次預測都需要計算與所有訓練樣本的距離，因此當訓練數據集較大時，計算開銷非常大，預測速度較慢。
- **存儲需求大**：K-NN需要儲存整個訓練集，對內存要求較高。
- **對特徵空間的尺度敏感**：K-NN對特徵的尺度比較敏感，需要對數據進行標準化或正規化處理，否則距離計算可能受到某些特徵的支配。

### 考點

1. **選擇距離度量方法**：
    
    - 根據問題的特性選擇合適的距離度量方法（歐氏距離、曼哈頓距離等）。
    - 不同的距離度量會影響預測結果，特別是在特徵空間的維度較高時。
2. **K值的選擇**：
    
    - K的選擇非常關鍵。K值較小可能會導致模型對訓練集過擬合，K值較大則可能導致欠擬合。
    - 通過交叉驗證（cross-validation）來選擇最佳的K值。
3. **數據的預處理**：
    
    - 在使用K-NN算法前，特徵縮放（如標準化或正規化）是非常重要的，特別是當特徵的單位和範圍差異較大時。
4. **適用場景**：
    
    - K-NN適用於小規模數據集和特徵空間較低的情況，對於大規模數據集和高維數據集，計算成本高，因此不太適用。
    - K-NN適用於對數據的分佈沒有明確假設的情況，特別是當數據存在複雜的非線性關係時。
5. **過擬合與欠擬合**：
    
    - K值的選擇影響模型的擬合程度。小的K值可能會導致過擬合，而大的K值則可能會導致欠擬合。
6. **空間複雜度**：
    
    - 由於K-NN需要存儲整個訓練集，因此它的空間複雜度是 O(n)O(n)，其中n是訓練樣本的數量。

### 總結

K近鄰算法是一種簡單且強大的監督學習算法，特別適用於小規模數據集和特徵空間較低的情況。通過選擇合適的距離度量方法和K值，它能夠有效地進行分類和回歸預測。雖然K-NN算法簡單易用，但它在面對大規模數據集和高維數據時的計算開銷較大，因此需要謹慎選擇其應用場景。