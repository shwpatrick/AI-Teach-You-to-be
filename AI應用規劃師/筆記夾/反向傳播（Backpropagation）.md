### **反向傳播（Backpropagation）基礎知識**：

**反向傳播**是深度學習中最重要的訓練算法之一，用於神經網絡的優化過程。其主要目的是根據損失函數的梯度來調整模型參數（權重和偏置），以使模型的預測結果更精確。

#### **反向傳播的基本概念**：

1. **前向傳播（Forward Propagation）**：
    
    - 在神經網絡中，前向傳播是指將輸入數據逐層傳遞到輸出層的過程，並通過激活函數生成預測結果。
    - 每一層的輸出是根據上一層的輸出和當前層的權重及偏置計算得來的。
2. **損失計算（Loss Calculation）**：
    
    - 根據預測結果和實際標籤計算損失函數的值，這是反向傳播的起點。
3. **反向傳播（Backpropagation）**：
    
    - 反向傳播通過鏈式法則計算每一層的損失對權重和偏置的梯度（即導數），然後將這些梯度傳回去，逐層更新權重和偏置。
    - 具體過程是：從輸出層開始，根據損失函數的梯度計算每一層的權重梯度，再將這些梯度反向傳遞到上一層。
4. **梯度下降（Gradient Descent）**：
    
    - 反向傳播的核心是梯度下降算法。每一層的權重和偏置是根據梯度來進行更新的，目的是最小化損失函數。
    - ![[更新公式-梯度下降.png]]

#### **反向傳播的步驟**：

1. **初始化權重和偏置**：
    
    - 通常使用隨機小值來初始化權重，偏置可以初始化為零或小的常數。
2. **前向傳播**：
    
    - 輸入數據通過網絡的每一層進行傳遞，計算每一層的輸出，最終獲得預測值。
3. **計算損失**：
    
    - 根據預測結果和真實值計算損失函數。
4. **反向傳播**：
    
    - 計算損失函數對每一層的權重和偏置的偏導數（梯度），然後根據這些梯度進行權重更新。
5. **權重更新**：
    
    - 使用梯度下降法或其變種（如隨機梯度下降、動量法、Adam等）來更新權重。
6. **重複以上過程**：
    
    - 重複步驟 2 到 5，直到模型收斂，即損失函數達到最小值。

#### **反向傳播的核心數學原理**：

- **鏈式法則（Chain Rule）**：反向傳播的關鍵在於利用鏈式法則計算每一層的梯度。這使得我們可以將損失對每一層參數的梯度分解成多個簡單的梯度計算，並逐層更新權重。
    
    假設損失函數 LLL 依賴於第 nnn 層的輸出 ana_nan​，而 ana_nan​ 依賴於第 n−1n-1n−1 層的輸出 an−1a_{n-1}an−1​，反向傳播的計算是基於鏈式法則：
    
    ![[鏈式法則.png]]
    

#### **反向傳播中的常見挑戰**：

1. **梯度消失問題**：
    
    - 在深層神經網絡中，梯度在多次傳遞後可能會變得非常小，導致權重更新極慢，從而難以訓練。這通常發生在使用 Sigmoid 或 Tanh 等激活函數時。
    - 解決方法：使用 ReLU 激活函數，或對梯度進行正則化。
2. **梯度爆炸問題**：
    
    - 如果梯度過大，權重更新過大，可能會導致模型不穩定，甚至發散。這通常出現在較深的網絡中。
    - 解決方法：使用梯度裁剪（Gradient Clipping）來限制梯度的最大值。
3. **學習率選擇**：
    
    - 學習率過大可能導致錯過最優解，過小則會使訓練速度變慢。
    - 解決方法：使用動態學習率調整方法（如 Adam、Adagrad）。

#### **考點**：

1. **反向傳播的基本概念與流程**：
    
    - 理解反向傳播的整體流程（前向傳播 → 損失計算 → 反向傳播 → 權重更新）。
2. **梯度計算與鏈式法則**：
    
    - 掌握如何使用鏈式法則計算每一層的梯度，理解反向傳播中的梯度計算過程。
3. **反向傳播與梯度下降的關係**：
    
    - 理解反向傳播是如何與梯度下降算法結合，用於更新神經網絡中的權重和偏置。
4. **常見問題與解決方案**：
    
    - 知道如何處理梯度消失與梯度爆炸問題，理解不同激活函數的選擇對反向傳播的影響。
5. **激活函數的選擇**：
    
    - 理解不同激活函數（如 Sigmoid、Tanh、ReLU）的優缺點，以及它們對反向傳播的影響。
6. **深度神經網絡的優化技術**：
    
    - 熟悉常見的優化方法（如隨機梯度下降、動量法、Adam 等）及其對反向傳播的應用。

### **總結**：

反向傳播是神經網絡訓練中的核心算法，通過計算每層的梯度並更新權重，最終使模型的預測結果與實際結果更為接近。掌握反向傳播的數學原理和常見挑戰，對於理解和應用深度學習模型至關重要。