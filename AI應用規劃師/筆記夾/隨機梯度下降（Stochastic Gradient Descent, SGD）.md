**隨機梯度下降（Stochastic Gradient Descent, SGD）** 是一種常用的優化算法，用於訓練機器學習模型，特別是在深度學習中。它是梯度下降法的變種，目的是最小化模型的損失函數，通過逐步更新模型參數來達到最優解。

### 1. **基本概念**

- **梯度下降法**（Gradient Descent）：這是一種尋找函數最小值的方法。在機器學習中，梯度下降用來最小化損失函數（例如均方誤差或交叉熵），進而優化模型的參數。其基本思想是：通過計算損失函數的梯度，並沿著梯度的反方向更新參數。
    
- **隨機梯度下降**（SGD）：與批量梯度下降（Batch Gradient Descent）不同，隨機梯度下降在每次更新中只用一個訓練樣本來計算梯度，而不是使用整個訓練集。這使得SGD比批量梯度下降更快，尤其是在處理大規模數據集時。
    

### 2. **數學表達式**

假設我們有一個損失函數 L(θ)L(\theta)L(θ) 和參數 θ\thetaθ，隨機梯度下降的更新規則如下：

θ=θ−η∇θL(θ,x(i),y(i))\theta = \theta - \eta \nabla_\theta L(\theta, x^{(i)}, y^{(i)})θ=θ−η∇θ​L(θ,x(i),y(i))

其中：

- θ\thetaθ 是模型的參數。
- η\etaη 是學習率（步長），控制每次更新的幅度。
- ∇θL(θ,x(i),y(i))\nabla_\theta L(\theta, x^{(i)}, y^{(i)})∇θ​L(θ,x(i),y(i)) 是針對第 iii 個訓練樣本的損失函數的梯度。

### 3. **SGD的過程**

1. **隨機選擇樣本**：從訓練集 {(x(1),y(1)),(x(2),y(2)),…,(x(n),y(n))}\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(n)}, y^{(n)})\}{(x(1),y(1)),(x(2),y(2)),…,(x(n),y(n))} 中隨機選擇一個樣本 (x(i),y(i))(x^{(i)}, y^{(i)})(x(i),y(i))。
2. **計算梯度**：根據當前的參數 θ\thetaθ，計算損失函數對參數的梯度 ∇θL(θ,x(i),y(i))\nabla_\theta L(\theta, x^{(i)}, y^{(i)})∇θ​L(θ,x(i),y(i))。
3. **更新參數**：通過更新公式 θ=θ−η∇θL(θ,x(i),y(i))\theta = \theta - \eta \nabla_\theta L(\theta, x^{(i)}, y^{(i)})θ=θ−η∇θ​L(θ,x(i),y(i)) 更新參數。
4. **重複過程**：對所有樣本進行多次更新，直到達到預設的停止條件（如達到最大迭代次數或損失函數收斂）。

### 4. **優勢與缺點**

#### 優勢：

- **計算效率高**：由於每次更新只使用一個樣本，因此每次參數更新所需的計算量較小，能夠更快地進行模型訓練，特別是在大規模數據集上。
- **能夠逃脫局部最小值**：隨機選擇樣本的方式使得參數更新具有隨機性，有助於避免困在局部最小值，尤其在非凸問題中可能更有優勢。

#### 缺點：

- **更新不穩定**：由於每次僅基於單個樣本計算梯度，因此更新方向具有隨機性，可能會導致收斂過程較為不穩定，甚至無法達到全局最小值。
- **收斂慢**：雖然每次更新速度較快，但隨著時間推移，收斂速度會變得較慢，尤其是在接近最小值時。

### 5. **改進方法**

為了解決SGD的缺點，研究者提出了幾種改進方法，常見的有：

- **小批量梯度下降（Mini-batch Gradient Descent）**：將訓練數據集分成多個小批次，每次從小批次中選擇一組樣本來計算梯度並更新參數。這樣做可以在計算效率和收斂穩定性之間找到平衡。
- **動量法（Momentum）**：這是一種基於過去梯度的加權平均來進行參數更新的技術，可以加速SGD的收斂，並幫助模型跳過局部最小值。
- **自適應學習率算法**：如Adagrad、RMSprop、Adam等，它們通過自動調整學習率來克服學習率選擇的問題。

### 6. **考點總結**

- **基本概念**：隨機梯度下降是通過每次使用單個樣本來計算梯度並更新模型參數，進而最小化損失函數。
- **更新規則**：每次更新模型參數時，只基於一個隨機選擇的樣本的梯度來進行更新。
- **優缺點**：SGD具有計算效率高、能夠逃脫局部最小值等優點，但也存在更新不穩定和收斂慢的缺點。
- **改進方法**：小批量梯度下降、動量法和自適應學習率算法等是常見的SGD改進方法。

SGD作為優化算法的基礎知識，對於深度學習和其他機器學習模型的訓練非常重要，能夠幫助選擇合適的更新方法來加速模型收斂。