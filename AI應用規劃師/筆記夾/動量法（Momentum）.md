### 動量法（Momentum）基礎知識與考點

#### **1. 動量法（Momentum）的基本概念**

動量法（Momentum）是一種用於優化梯度下降的方法，透過引入 **動量（Momentum）** 來加速收斂，並減少震盪問題，特別適用於深度神經網絡的訓練。

在標準的梯度下降（Gradient Descent）中，每次更新權重時，僅依賴當前梯度的方向：

![[標準梯度下降.png]]

然而，標準梯度下降在非凸函數或凹凸面中可能會出現震盪，特別是在梯度變化劇烈的情況下（如鞍點或陡峭區域）。

為了解決這個問題，**動量法引入了一個動量項**，類似於物理中的動量概念，使得更新方向不僅依賴於當前梯度，還受過去梯度的影響，從而能夠平滑梯度變化，並加速收斂。

---

#### **2. 動量法的數學公式**

動量法在參數更新時，引入一個指數衰減的過去梯度累積項：

![[動量法參數更新.png]]

**直覺理解：**

- 動量項 vtv_t 相當於一個「速度」，用來記錄梯度的歷史信息，並影響當前的更新。
- 當梯度方向一致時，累積的動量會讓更新速度變快，加速收斂。
- 當梯度方向變化劇烈時（如震盪區域），動量可以平滑更新，減少梯度方向的劇烈擺動。

---

#### **3. 動量法的優勢**

- **加速收斂**：透過累積梯度的歷史信息，能夠更快地移動到最優點，特別是在凹凸函數（如鞍點）附近。
- **減少震盪**：標準梯度下降容易因梯度變化劇烈而震盪，而動量法能夠減少這種震盪，使得更新更加穩定。
- **適用於深度學習**：深度神經網絡的損失函數通常較為複雜，動量法能夠幫助神經網絡更快地學習有效的權重。

---

#### **4. 動量法的變種**

1. **Nesterov 加速梯度法（NAG, Nesterov Accelerated Gradient）**
    
    - Nesterov 方法在計算梯度時，先根據當前的動量估計下一步的參數，再計算梯度：
    
    vt=γvt−1+η∇J(θt−γvt−1)v_t = \gamma v_{t-1} + \eta \nabla J(\theta_t - \gamma v_{t-1}) θt+1=θt−vt\theta_{t+1} = \theta_t - v_t
    - 這樣做的優勢是能夠提前「預測」下一步的變化，進一步減少震盪。
2. **RMSprop + Momentum**
    
    - RMSprop 是另一種優化方法（自適應學習率），結合 Momentum 後，可以更穩定地在不同方向上調整步長。
3. **Adam（Adaptive Moment Estimation）**
    
    - Adam 方法結合了 Momentum 和 RMSprop，使其在不同類型的問題上都能表現良好：
    
    mt=β1mt−1+(1−β1)∇J(θt)m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla J(\theta_t) vt=β2vt−1+(1−β2)(∇J(θt))2v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla J(\theta_t))^2 θt+1=θt−ηvt+ϵmt\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
    - 其中 mtm_t 是一階動量，vtv_t 是二階動量。

---

#### **5. 動量法的考點**

在面試或考試中，可能會遇到以下考點：

1. **動量法的數學公式**
    
    - 需要熟悉標準動量法的公式，以及如何利用動量累積梯度。
    - Nesterov 動量的公式與標準動量法的區別。
2. **動量係數 γ\gamma 的作用**
    
    - γ\gamma 控制過去梯度對當前更新的影響，通常取 0.9 或 0.99。
    - 若 γ\gamma 太小，則無法充分利用過去的梯度信息；
    - 若 γ\gamma 太大，則可能會導致過衝（Overshooting）。
3. **動量法與標準梯度下降的對比**
    
    - 標準梯度下降更新只依賴當前梯度，而動量法累積了過去的梯度，使得更新更加平滑。
    - 動量法更適合非凸函數，能有效減少震盪，加速收斂。
4. **動量法與其他優化算法的比較**
    
    - 與 **NAG（Nesterov 加速梯度）** 的區別：NAG 會提前「預測」下一步的變化，使得梯度下降更加精確。
    - 與 **RMSprop、Adam** 的區別：Adam 結合了動量法和 RMSprop，能夠自適應學習率，適用於不同類型的問題。
5. **動量法的適用場景**
    
    - 適合深度神經網絡訓練，特別是在收斂較慢的情況下。
    - 適用於優化非凸損失函數，如影像分類、語音識別等領域的深度學習模型。

---

### **小結**

- **動量法（Momentum）** 是一種改進梯度下降的方法，能夠加速收斂並減少梯度震盪。
- **數學公式**：更新時累積過去梯度，使得更新方向更加穩定。
- **優勢**：加速收斂、減少震盪，適用於深度學習。
- **考點**：公式推導、動量係數影響、與其他優化算法的比較、適用場景。

這些概念在面試和考試中都是重點，需要熟練掌握！